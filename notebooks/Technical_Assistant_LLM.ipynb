{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7-5jh85qdpG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Configuration ---\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "dataset_id = \"b-mc2/sql-create-context\"\n",
        "# This will be the name of your model repository on the Hugging Face Hub\n",
        "new_model_name = \"phi-3-mini-sql-assistant\"\n",
        "\n",
        "# --- Load Dataset ---\n",
        "# We load the dataset and then, for this educational project, we'll shuffle it\n",
        "# and select a smaller portion to make training faster.\n",
        "dataset = load_dataset(dataset_id, split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(10000)) # Using 10k examples for speed\n",
        "\n",
        "# Split our data into a training set and a small test set to check our work later\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "print(f\"Training set size: {len(dataset['train'])}\")\n",
        "print(f\"Test set size: {len(dataset['test'])}\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "# A small but important detail: we need to add a padding token.\n",
        "# We'll set it to the end-of-sequence token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Create the Prompt Template ---\n",
        "def format_prompt(example):\n",
        "    # This function creates a single string that follows the Phi-3 chat template\n",
        "    prompt = f\"\"\"<|user|>\n",
        "Given the database schema:\n",
        "{example['context']}\n",
        "\n",
        "Generate the SQL query for the following request:\n",
        "{example['question']}<|end|>\n",
        "<|assistant|>\n",
        "{example['answer']}<|end|>\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Now, apply this formatting to our entire dataset\n",
        "train_dataset = dataset[\"train\"].map(format_prompt)\n",
        "test_dataset = dataset[\"test\"].map(format_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5FvZoWCqiwh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Configuration ---\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "dataset_id = \"b-mc2/sql-create-context\"\n",
        "# This will be the name of your model repository on the Hugging Face Hub\n",
        "new_model_name = \"phi-3-mini-sql-assistant\"\n",
        "\n",
        "# --- Load Dataset ---\n",
        "# We load the dataset and then, for this educational project, we'll shuffle it\n",
        "# and select a smaller portion to make training faster.\n",
        "dataset = load_dataset(dataset_id, split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(10000)) # Using 10k examples for speed\n",
        "\n",
        "# Split our data into a training set and a small test set to check our work later\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "print(f\"Training set size: {len(dataset['train'])}\")\n",
        "print(f\"Test set size: {len(dataset['test'])}\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "# A small but important detail: we need to add a padding token.\n",
        "# We'll set it to the end-of-sequence token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Create the Prompt Template ---\n",
        "def format_prompt(example):\n",
        "    # This function creates a single string that follows the Phi-3 chat template\n",
        "    prompt = f\"\"\"<|user|>\n",
        "Given the database schema:\n",
        "{example['context']}\n",
        "\n",
        "Generate the SQL query for the following request:\n",
        "{example['question']}<|end|>\n",
        "<|assistant|>\n",
        "{example['answer']}<|end|>\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Now, apply this formatting to our entire dataset\n",
        "train_dataset = dataset[\"train\"].map(format_prompt)\n",
        "test_dataset = dataset[\"test\"].map(format_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zLy44wW3rj8R"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import gc # Import garbage collector\n",
        "\n",
        "# --- Clear GPU memory before loading model ---\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Attempting to clear GPU memory before model load.\")\n",
        "\n",
        "# --- 1. Configure Quantization (Shrinking the model) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- 2. Load the Base Model ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    # Removed device_map=\"auto\" as it's causing conflicts with quantization and offloading\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False # Important for training\n",
        "\n",
        "# --- 3. Configure LoRA (The Adapters) ---\n",
        "peft_config = LoraConfig(\n",
        "    r=8, # Rank of the adapters. A higher rank means more parameters to train.\n",
        "    lora_alpha=16, # A scaling factor for the adapters.\n",
        "    lora_dropout=0.1, # Helps prevent overfitting.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # We target these specific modules in the Phi-3 architecture\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Prepare the model for training with PEFT\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# --- 4. Configure Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4, # Simulates a larger batch size (2*4=8)\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# --- 5. Create and Run the Trainer ---\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# Let's train!\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# --- 6. Save and Push Your Work ---\n",
        "print(\"Saving the adapter to the Hub...\")\n",
        "trainer.model.push_to_hub(new_model_name, use_auth_token=True)\n",
        "tokenizer.push_to_hub(new_model_name, use_auth_token=True)\n",
        "print(\"All done! Your model is saved online.\")\n",
        "\n",
        "# --- Explicitly clear GPU memory after training ---\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"GPU memory cleared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKsGKtL8sCHz"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig # Import BitsAndBytesConfig\n",
        "import random\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "print(\"\\n--- Testing the fine-tuned model ---\")\n",
        "\n",
        "# --- Re-define Quantization Configuration for Inference ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- Load the fine-tuned model ---\n",
        "# AutoPeftModelForCausalLM automatically loads the base model and applies the adapter.\n",
        "full_model_name = \"manuelaschrittwieser/phi-3-mini-sql-assistant\"\n",
        "\n",
        "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    full_model_name,\n",
        "    quantization_config=bnb_config, # Pass the quantization config here\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    dtype=torch.float16\n",
        ")\n",
        "\n",
        "# --- Merge the adapter into the base model ---\n",
        "merged_model = trained_model.merge_and_unload()\n",
        "\n",
        "# --- Test with a random example from the test set ---\n",
        "sample = dataset[\"test\"][random.randint(0, len(dataset[\"test\"])-1)]\n",
        "context = sample['context']\n",
        "question = sample['question']\n",
        "ground_truth_sql = sample['answer']\n",
        "\n",
        "# Create the prompt for inference (without the answer!)\n",
        "prompt = f\"\"\"<|user|>\n",
        "Given the database schema:\n",
        "{context}\n",
        "\n",
        "Generate the SQL query for the following request:\n",
        "{question}<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the prompt and send it to the GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# Generate the output\n",
        "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=False, use_cache=False)\n",
        "\n",
        "# Decode only the newly generated tokens (after the input_ids)\n",
        "generated_output_tokens = outputs[0][len(input_ids[0]):]\n",
        "generated_sql = tokenizer.decode(generated_output_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "# --- Post-process generated SQL to extract only the query ---\n",
        "# Remove conversational text and markdown code block if present\n",
        "if \"```sql\" in generated_sql:\n",
        "    generated_sql = generated_sql.split(\"```sql\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "# Remove any lingering <|end|> tokens if present at the end of the generated SQL\n",
        "if generated_sql.endswith(\"<|end|>\"):\n",
        "    generated_sql = generated_sql[:-len(\"<|end|>\")].strip()\n",
        "\n",
        "# --- Print the results ---\n",
        "print(f\"Schema:\\n{context}\")\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"\\nâœ… Ground Truth SQL: {ground_truth_sql}\")\n",
        "print(f\"ðŸ¤– Generated SQL:    {generated_sql}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}